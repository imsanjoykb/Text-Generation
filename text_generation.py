# -*- coding: utf-8 -*-
"""Text Generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QZY3TDCWwq6TMzOQh98623WXl44vJQSY

#### Author : Sanjoy Biswas
#### Email : sanjoy.eee32@gmail.com
### Portfolio : https://imsanjoykb.github.io/
"""

!pip install transformers

"""### Import Necessary Libraries"""

import numpy as np
import pandas as pd
import seaborn as sns
import urllib
import re
import os
import tensorflow as tf
from transformers import GPT2LMHeadModel, GPT2Tokenizer

"""### Import Tokenizer"""

## GPT-2 is a transformers model pretrained on a very large corpus of English data
## pretrained on the raw texts only, with no humans labelling them with autometic Process
## generating texts from a prompt

tokenizer = GPT2Tokenizer.from_pretrained("gpt2-large")

"""### Import Model"""

## GPT-2 stack of transformer decoders which makes it so powerful in generating meaningful texts

model = GPT2LMHeadModel.from_pretrained("gpt2-large", pad_token_id=tokenizer.eos_token_id)

"""### Set The Topic"""

## Letâ€™s take an example test text like

sentence = 'Cristiano Ronaldo match'

"""## Encode the input i.e. topic"""

## Tokenize this input text for GPT input i.e. converting text to integer indices

input_ids = tokenizer.encode(sentence, return_tensors='pt')

input_ids

## max_lenth-Number of Words in the Article
## num_beams-Number of different combination of words that can be chained together
## no_repeat_ngram_size-No of words that be combined together and repeated, example: ['benefits of sleeping' can be repeated 2 times but not more ]
## generate text until the output length (which includes the context length) reaches 50

output = model.generate(
    input_ids, 
    max_length=30, 
    num_beams=5, 
    no_repeat_ngram_size=2, 
    early_stopping=True
    )

## Tensor Output
output

## This is our tokenized text, if want to decode these indices values, can do so by decode method provided by the same tokenizer.
print(tokenizer.decode(output[0], skip_special_tokens=True))

"""### Save the output in a variable"""

text_x = tokenizer.decode(output[0], skip_special_tokens=True)

text = tokenizer.decode(output[0], skip_special_tokens=True).join(text_x.split(".")[:-1]) + "."

text

"""### Install & Import Gradio"""

!pip install -q gradio

import gradio as gr

def generate_text(sentence):
    input_ids = tokenizer.encode(sentence, return_tensors='pt')
    beam_output = model.generate(input_ids, max_length=30, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)
    output = tokenizer.decode(beam_output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
    return ".".join(output.split(".")[:-1]) + "."

"""### Creating Gradio Interface"""

output_text = gr.outputs.Textbox()
gr.Interface(generate_text,"textbox", output_text, title="Text Generation",
             description="Generate Text and Comment With AI System").launch()

"""### Save Output as CSV File"""

from google.colab import files
with open('generatefile.csv','w') as f:
  f.write(text)

"""### Generate Pickle File"""

import pickle
pickle.dump(model,open('nlp_model.pkl','wb'))

pickle.dump(text,open('projmodel.pkl','wb'))

